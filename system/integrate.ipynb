{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle\n",
    "import util\n",
    "import pandas as pd\n",
    "from entity_link.features import feature_select\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDetect(object):\n",
    "    def __init__(self):\n",
    "        self.nlp = StanfordCoreNLP(\"http://10.60.1.82\", port=9999, lang=\"en\")\n",
    "        print(\"Stanford CoreNLP Server connnected ...\")\n",
    "        self.tag_list = [\"FW\", \"NN\", \"NNP\", \"NNPS\", \"NNS\"]\n",
    "        self.tag_NN = [\"NN\", \"NNP\", \"NNPS\", \"NNS\"]\n",
    "        self.id2name = {}\n",
    "        self.name2id = {}\n",
    "        self.strict = True\n",
    "        self.keyword_strict = True\n",
    "        self.proportion_strict = True\n",
    "        if(self.strict):\n",
    "            self.keyword_strict = True\n",
    "            self.proportion_strict = True\n",
    "        \n",
    "    \n",
    "    def getKeyWords(self, question):\n",
    "        ## 一些专有名词便是直接大写化的\n",
    "        ## question = question.lower()\n",
    "        keyword_list = set()\n",
    "        \n",
    "        word_tag = self.nlp.pos_tag(question)\n",
    "        ner_tag = self.nlp.ner(question)\n",
    "        tag_length = len(word_tag)\n",
    "        ner_length = len(ner_tag)\n",
    "        \n",
    "        ## 从词性标注中添加单个关键词\n",
    "        for item in word_tag:\n",
    "            if(item[1] in self.tag_list):\n",
    "                keyword_list.add(item[0])\n",
    "                \n",
    "        ## 从词性标注中添加多个关键词\n",
    "        for i in range(tag_length):\n",
    "            if(word_tag[i][1] == \"FW\"):\n",
    "                string = \"\"\n",
    "                while(i < tag_length and word_tag[i][1] == \"FW\"):\n",
    "                    string =  string + word_tag[i][0] + \" \"\n",
    "                    i = i + 1\n",
    "                keyword_list.add(string.rstrip(\" \"))\n",
    "\n",
    "            if(i < tag_length and word_tag[i][1] in self.tag_NN):\n",
    "                string = \"\"\n",
    "                while(i < tag_length and word_tag[i][1] in self.tag_NN):\n",
    "                    string =  string + word_tag[i][0] + \" \"\n",
    "                    i = i + 1\n",
    "                keyword_list.add(string.rstrip(\" \"))\n",
    "\n",
    "        ## 从命名实体识别中添加单个关键词\n",
    "        for item in ner_tag:\n",
    "            if(item[1] != \"O\"):\n",
    "                keyword_list.add(item[0])\n",
    "\n",
    "        \n",
    "        ## 从命名实体识别中添加多个关键词\n",
    "        for i in range(ner_length):\n",
    "            if(ner_tag[i][1] != \"O\"):\n",
    "                tag = ner_tag[i][1] \n",
    "                string = \"\"\n",
    "                while(i < ner_length and ner_tag[i][1] == tag):\n",
    "                    string =  string + ner_tag[i][0] + \" \"\n",
    "                    i = i + 1\n",
    "                keyword_list.add(string.rstrip(\" \"))\n",
    "                \n",
    "        print(\"ori_keyword_list: \"+ str(keyword_list))\n",
    "        if(self.keyword_strict):\n",
    "            tmp_list = keyword_list.copy()\n",
    "            for item in tmp_list:\n",
    "                inflag = False\n",
    "                for word in keyword_list:\n",
    "                    if(item in word and item != word):\n",
    "                        inflag = True\n",
    "                        break\n",
    "                if(inflag):\n",
    "                    keyword_list.remove(item)\n",
    "        print(\"cur_keyword_list: \"+ str(keyword_list))         \n",
    "        return keyword_list\n",
    "    \n",
    "    def loadData(self, filepath):\n",
    "        count = 0\n",
    "        with open(filepath,\"r\",encoding=\"UTF-8\") as file:\n",
    "            for line in file:\n",
    "                count = count + 1\n",
    "                if(count % 1000000 == 0):\n",
    "                    print(\"loaded %d entities ... \" % count)\n",
    "                index = line.find(\",\")\n",
    "                ID = line[:index]\n",
    "                name = line[index+1:-1]\n",
    "                self.id2name[ID] = name\n",
    "                if(name in self.name2id):\n",
    "                    self.name2id[name].append(ID)\n",
    "                else:\n",
    "                    self.name2id[name] = [ID]\n",
    "        file.close()\n",
    "        print(\"entity names loaded !\")\n",
    "        return self.id2name, self.name2id\n",
    "    \n",
    "    def IsInString(self, name, string):\n",
    "        regex = \"(^\" + name + \"$)|(^\" + name + \"\\W.*)|(.*\\W\" + name + \"\\W.*)|(.*\\W\" + name + \"$)\"\n",
    "        pattern = re.compile(regex)\n",
    "        if(pattern.match(string)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def ProportionStrict(self, substring, string):\n",
    "        if(self.proportion_strict == False):\n",
    "            return True\n",
    "        word_threshold = 0.3\n",
    "        char_threshold = 0.3\n",
    "        word_list = word_tokenize(string)\n",
    "        sub_word_list = word_tokenize(substring)\n",
    "        string_chars = sum([len(x) for x in word_list])\n",
    "        sub_string_chars = sum([len(x) for x in sub_word_list])\n",
    "        string_words = len(word_list)\n",
    "        sub_string_words = len(sub_word_list)\n",
    "        \n",
    "        return sub_string_words/string_words >= word_threshold and sub_string_chars/string_chars >= char_threshold\n",
    "        \n",
    "    \n",
    "    def DetectEntities(self,keywords,threshold=100000):\n",
    "        result = {\"topic_words\":[], \"topic_words_names\":[]}\n",
    "        keywords = [x.lower() for x in keywords]\n",
    "        for key in self.name2id.keys():\n",
    "            for word in keywords:\n",
    "                if(word in key and self.IsInString(word, key) and self.ProportionStrict(word, key)):\n",
    "                    for ID in self.name2id[key]:\n",
    "                        result['topic_words'].append(ID)\n",
    "                        result['topic_words_names'].append(key)\n",
    "                        if(len(result['topic_words']) > threshold):\n",
    "                            return result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QustionAnswering(object):\n",
    "    def __init__(self):\n",
    "        self.xgb = pickle.load(open(\"../datas/models/xgb_all.pickle.dat\", \"rb\"))\n",
    "        print(\"Model loaded ...\")\n",
    "        self.detect = EntityDetect()\n",
    "    \n",
    "    def load_data(self, filepath):\n",
    "        return self.detect.loadData(filepath)\n",
    "    \n",
    "    def set_data(self, id2name, name2id):\n",
    "        self.detect.id2name = id2name\n",
    "        self.detect.name2id = name2id\n",
    "    \n",
    "    def gen_item_features(self, input):\n",
    "        data = {'question': [], 'topic_words': [], 'word_score': [], 'topic_words_names': [], \"label\":[] }\n",
    "        assert len(input['topic_words']) == len(input['topic_words_names'])\n",
    "        cand_name = input['topic_words_names']\n",
    "        question = input['question'][0]\n",
    "        data['question'] = question\n",
    "        for i, cand in enumerate(input['topic_words']):\n",
    "            data['topic_words'].append(cand)\n",
    "            data['topic_words_names'].append(cand_name[i])\n",
    "            data['word_score'].append(1)\n",
    "            data['label'].append(1)\n",
    "            \n",
    "        df = pd.DataFrame(data)\n",
    "        features, _ = feature_select(df)\n",
    "        return features\n",
    "    \n",
    "    def get_cand_entities(self,question):\n",
    "        keywords = self.detect.getKeyWords(question)\n",
    "        print(\"keywords generated!\")\n",
    "        input = self.detect.DetectEntities(keywords)\n",
    "        input['question'] = question\n",
    "        input = pd.DataFrame(input)\n",
    "        print(\"candidates generated!\")\n",
    "        return input\n",
    "    \n",
    "    def get_top_entities(self, input):\n",
    "        features = self.gen_item_features(input)\n",
    "        print(\"features extracted!\")\n",
    "        predict = self.xgb.predict_proba(features)\n",
    "        print(\"scores generated!\")\n",
    "        input['predict'] = predict[:, 1]\n",
    "        head_100 = input.sort_values(['predict'], ascending=False).head(100)\n",
    "        return head_100\n",
    "    \n",
    "    def get_results(self, question):\n",
    "        input = self.get_cand_entities(question)\n",
    "        return self.get_top_entities(input)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded ...\n",
      "Stanford CoreNLP Server connnected ...\n",
      "loaded 1000000 entities ... \n",
      "entity names loaded !\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "QA = QustionAnswering()\n",
    "# (id2name,name2id) = QA.load_data(\"../datas/zyt/mid2name_finally.txt\")\n",
    "(id2name,name2id) = QA.load_data(\"../datas/zyt/FB2M_names.txt\")\n",
    "# QA.set_data(id2name,name2id)\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ori_keyword_list: {'yves', 'klein', 'death', 'cause', 'yves klein'}\n",
      "cur_keyword_list: {'death', 'cause', 'yves klein'}\n",
      "keywords generated!\n",
      "candidates generated!\n",
      "Time Used: 0.5987193584442139s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time1 = time.time()\n",
    "# question = \"What's the capital of United States?\"\n",
    "# question = \"What's the meaning of Junk Foods?\"\n",
    "# question = \"Slogan of Communist Party of China?\"\n",
    "question = \"what was the cause of death of yves klein?\"\n",
    "cand = QA.get_cand_entities(question)\n",
    "time2 = time.time()\n",
    "print(\"Time Used: \" + str(time2-time1) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'was', 'the', 'cause', 'of', 'death', 'of', 'yves', 'klein', '?']\n",
      "[('what', 'WP'), ('was', 'VBD'), ('the', 'DT'), ('cause', 'NN'), ('of', 'IN'), ('death', 'NN'), ('of', 'IN'), ('yves', 'NNS'), ('klein', 'NN'), ('?', '.')]\n",
      "[('what', 'O'), ('was', 'O'), ('the', 'O'), ('cause', 'O'), ('of', 'O'), ('death', 'O'), ('of', 'O'), ('yves', 'O'), ('klein', 'O'), ('?', 'O')]\n"
     ]
    }
   ],
   "source": [
    "question = \"what was the cause of death of yves klein?\"\n",
    "print(QA.detect.nlp.word_tokenize(question))\n",
    "print(QA.detect.nlp.pos_tag(question))\n",
    "print(QA.detect.nlp.ner(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword_list: {'Slogan', 'Communist Party of China'}\n",
      "keywords generated!\n",
      "candidates generated!\n",
      "features extracted!\n",
      "scores generated!\n"
     ]
    }
   ],
   "source": [
    "res = QA.get_results(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>topic_words_names</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slogan of Communist Party of China?</td>\n",
       "      <td>/m/02189</td>\n",
       "      <td>Communist Party of China</td>\n",
       "      <td>0.277958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slogan of Communist Party of China?</td>\n",
       "      <td>/m/0crtpxt</td>\n",
       "      <td>Slogan</td>\n",
       "      <td>0.005385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question topic_words         topic_words_names  \\\n",
       "0  Slogan of Communist Party of China?    /m/02189  Communist Party of China   \n",
       "1  Slogan of Communist Party of China?  /m/0crtpxt                    Slogan   \n",
       "\n",
       "    predict  \n",
       "0  0.277958  \n",
       "1  0.005385  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
