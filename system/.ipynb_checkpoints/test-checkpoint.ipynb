{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle\n",
    "import util\n",
    "import pandas as pd\n",
    "from entity_link.features import feature_select\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDetect(object):\n",
    "    def __init__(self):\n",
    "        self.nlp = StanfordCoreNLP(\"http://10.60.1.82\", port=9999, lang=\"en\")\n",
    "        print(\"Stanford CoreNLP Server connnected ...\")\n",
    "        self.tag_list = [\"FW\", \"NN\", \"NNP\", \"NNPS\", \"NNS\"]\n",
    "        self.tag_NN = [\"NN\", \"NNP\", \"NNPS\", \"NNS\", \"JJ\"]\n",
    "        self.keyword_strict = True      \n",
    "    \n",
    "    def getKeyWords(self, question):\n",
    "        keyword_list = set()\n",
    "    \n",
    "        word_tag = self.nlp.pos_tag(question)\n",
    "        ner_tag = self.nlp.ner(question)\n",
    "        tag_length = len(word_tag)\n",
    "        ner_length = len(ner_tag)\n",
    "        \n",
    "        print(word_tag)\n",
    "        print(ner_tag)\n",
    "        \n",
    "        ## 从词性标注中添加单个关键词\n",
    "        for item in word_tag:\n",
    "            if(item[1] in self.tag_list):\n",
    "                keyword_list.add(item[0])\n",
    "                \n",
    "        ## 从词性标注中添加多个关键词\n",
    "        for i in range(tag_length):\n",
    "            if(word_tag[i][1] == \"FW\"):\n",
    "                string = \"\"\n",
    "                while(i < tag_length and word_tag[i][1] == \"FW\"):\n",
    "                    string =  string + word_tag[i][0] + \" \"\n",
    "                    i = i + 1\n",
    "                keyword_list.add(string.rstrip(\" \"))\n",
    "            \n",
    "            if(i < tag_length and word_tag[i][1] in self.tag_NN):\n",
    "                string = \"\"\n",
    "                while(i < tag_length and word_tag[i][1] in self.tag_NN):\n",
    "                    string =  string + word_tag[i][0] + \" \"\n",
    "                    i = i + 1\n",
    "                keyword_list.add(string.rstrip(\" \"))\n",
    "            \n",
    "\n",
    "        ## 从命名实体识别中添加单个关键词\n",
    "        for item in ner_tag:\n",
    "            if(item[1] != \"O\"):\n",
    "                keyword_list.add(item[0])\n",
    "        \n",
    "        ## 从命名实体识别中添加多个关键词\n",
    "        for i in range(ner_length):\n",
    "            if(ner_tag[i][1] != \"O\"):\n",
    "                tag = ner_tag[i][1] \n",
    "                string = \"\"\n",
    "                while(i < ner_length and ner_tag[i][1] == tag):\n",
    "                    string =  string + ner_tag[i][0] + \" \"\n",
    "                    i = i + 1\n",
    "                keyword_list.add(string.rstrip(\" \"))\n",
    "                \n",
    "        if(self.keyword_strict):\n",
    "            tmp_list = keyword_list.copy()\n",
    "            for item in tmp_list:\n",
    "                inflag = False\n",
    "                for word in keyword_list:\n",
    "                    if(item in word and item != word):\n",
    "                        inflag = True\n",
    "                        break\n",
    "                if(inflag):\n",
    "                    keyword_list.remove(item)\n",
    "    \n",
    "        return keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = EntityDetect()\n",
    "question = \"which indian city was fazil born in\"\n",
    "nlp.getKeyWords(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "wordsBag = []\n",
    "newfile = open(\"./WordOfBag.txt\",\"w\",encoding=\"UTF-8\")\n",
    "with open(\"./questions_all.txt\",\"r\", encoding=\"UTF-8\") as file:\n",
    "    for line in file:\n",
    "        count = count + 1\n",
    "        if(count % 100 == 0):\n",
    "            print(\"line-%d ...\" % count)\n",
    "        question = line[1:-2]\n",
    "        keywords = nlp.getKeyWords(question)\n",
    "        for word in keywords:\n",
    "            wordsBag.append(word)\n",
    "print(\"Finished!\")\n",
    "for item in wordsBag:\n",
    "    newfile.write(item+\"\\n\")\n",
    "print(\"File created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "with open(\"./wordBag.txt\",\"r\",encoding=\"UTF-8\") as file:\n",
    "    for line in file:\n",
    "        word = line[:-1]\n",
    "        if(word in dic):\n",
    "            dic[word] = dic[word] + 1\n",
    "        else:\n",
    "            dic[word] = 1\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = sorted(dic.items(), key=lambda item:item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(dic, open(\"wordbag.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newfile = open(\"newquestions.txt\",\"w\",encoding=\"UTF-8\")\n",
    "with open(\"./questions_all.txt\",\"r\",encoding=\"UTF-8\") as file:\n",
    "    for line in file:\n",
    "        newfile.write(line[1:-2] + \"\\n\")\n",
    "print(\"finished!\")\n",
    "newfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "newdata = json.load(open(\"../datas/zyt/keyword_frequence.json\",\"r\",encoding=\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "def word_count(string1, string2):\n",
    "    count = 0\n",
    "    list1 = word_tokenize(string1)\n",
    "    for word in list1:\n",
    "        count = count + string2.count(word)\n",
    "    return count    \n",
    "\n",
    "\n",
    "def get_top_relation(keywords, relations):\n",
    "    if(relations == None or keywords == None):\n",
    "        return None\n",
    "   \n",
    "    rel_set = set()\n",
    "    for rel in relations:\n",
    "        rel_set.add(rel)\n",
    "    \n",
    "    rel_scores = {}\n",
    "    for rel in rel_set:\n",
    "        ## 如果是Wikidata：仅对英文名计算相似度\n",
    "        # word_list = word_tokenize(rel.split(\"#\")[0])\n",
    "        ## 如果是Freebase：需要对每个单词进行处理，并去除开头的 /\n",
    "        word_list = re.split('[_|/]', rel[1:])\n",
    "        rel_len = len(word_list)\n",
    "        rel_scores[rel] = sorted([word_count(x, rel)/rel_len for x in keywords], reverse=True)[0]\n",
    "    \n",
    "    ## 根据分数进行排序，得到元组列表，第一个元组中包含关系名和分数\n",
    "    relation_rank = sorted(rel_scores.items(), key=lambda item:item[1], reverse=True)\n",
    "    print(\"---------- relation rank below ----------\")\n",
    "    print(\"relations: \" + str(relation_rank))\n",
    "    print(\"---------- relation rank above ----------\")\n",
    "    return relation_rank[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = set(['cyclone', 'hainan'])\n",
    "relations = ['/base/aareas/schema/administrative_area/administrative_area_type',  '/location/location/people_born_here',  '/base/aareas/schema/administrative_area/administrative_parent',  '/location/location/events',  '/location/cn_province/capital',  '/location/location/containedby',  '/meteorology/cyclone_affected_area/cyclones',  '/location/administrative_division/country',  '/common/topic/notable_types',  '/location/location/contains',  '/base/aareas/schema/administrative_area/administrative_children',  '/base/aareas/schema/administrative_area/capital']\n",
    "get_top_relation(keywords, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wikiparser import getMainImage, infoBox\n",
    "getMainImage(\"https://en.wikipedia.org/wiki?curid=2723452\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 需要进行改进的地方有三处\n",
    "+ 对一些常用词进行mapping(可以认为是模型学习得到)\n",
    "```\n",
    "    kind: type, genre, label\n",
    "    release: track, album, music, song \n",
    "    film: produce, direct, director\n",
    "    people: person\n",
    "    capital: city\n",
    "    pass away: die, death\n",
    "    nationality: country, where from \n",
    "```\n",
    "+ 对关键词进行词干提取的处理\n",
    "```python    \n",
    "    import nltk.stem  \n",
    "    s = nltk.stem.SnowballStemmer('english')  \n",
    "    s.stem('imaging')\n",
    "```\n",
    "+ 如果无法找到关系，可以通过同义词的方式处理\n",
    "```python\n",
    "    ## 同义词中的 synset 三元组是(词.词性.序号)\n",
    "    from nltk.corpus import wordnet\n",
    "    syns = wordnet.synsets(\"word\")\n",
    "    syns = wordnet.synsets(\"word\", pos=wordnet.VERB)\n",
    "    ## 其中一些 pos 如下：\n",
    "    ADJ = 'a'  ADJ_SAT = 's'  ADV = 'r'  NOUN = 'n'  VERB = 'v' \n",
    "```\n",
    "\n",
    "+ 训练词向量，通过embedding的相似度计算来得到top relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a class of art (or artistic endeavor) having a characteristic form or technique\n",
      "a kind of literary or artistic work\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"genre\")\n",
    "a = wordnet.synset('genre.n.04')\n",
    "b = wordnet.synset('genre.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem  \n",
    "s = nltk.stem.SnowballStemmer('english')  \n",
    "s.stem('director')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
